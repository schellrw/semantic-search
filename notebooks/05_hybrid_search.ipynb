{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\schel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\schel\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import lancedb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from scipy.sparse import load_npz\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from src.evaluation.metrics import calculate_hits_at_k, calculate_mrr, evaluate_search_system\n",
        "from src.text_processing.cleaning import clean_text_light, clean_text_moderate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline dataset: (519, 11)\n",
            "Imputed dataset: (519, 11)\n",
            "\n",
            "Keyword artifacts loaded:\n",
            "Light keywords: 150\n",
            "Moderate keywords: 150\n"
          ]
        }
      ],
      "source": [
        "# Load baseline and imputed datasets\n",
        "df_baseline = pd.read_parquet(\"../data/processed/df_baseline_clean.parquet\")\n",
        "df_imputed = pd.read_parquet(\"../data/processed/df_imputed_clean.parquet\")\n",
        "\n",
        "print(f\"Baseline dataset: {df_baseline.shape}\")\n",
        "print(f\"Imputed dataset: {df_imputed.shape}\")\n",
        "\n",
        "# Load keyword artifacts - don't load raw since has many stopwords\n",
        "with open(\"../src/text_processing/artifacts/light_keywords.json\", \"r\") as f:\n",
        "    light_keywords = json.load(f)\n",
        "\n",
        "with open(\"../src/text_processing/artifacts/moderate_keywords.json\", \"r\") as f:\n",
        "    moderate_keywords = json.load(f)\n",
        "\n",
        "print(f\"\\nKeyword artifacts loaded:\")\n",
        "print(f\"Light keywords: {len(light_keywords)}\")\n",
        "print(f\"Moderate keywords: {len(moderate_keywords)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating datasets with different text combination strategies...\n"
          ]
        }
      ],
      "source": [
        "# Evaluate different text combination strategies\n",
        "def create_combined_text_v1(row):\n",
        "    \"\"\"Original strategy: title + description + brand + bullets + color\"\"\"\n",
        "    parts = [\n",
        "        str(row.get('product_title', '')),\n",
        "        str(row.get('product_description', '')),\n",
        "        str(row.get('product_brand', '')),\n",
        "        str(row.get('product_bullet_point', '')),\n",
        "        str(row.get('product_color', ''))\n",
        "    ]\n",
        "    return ' '.join([p for p in parts if p and p.lower() not in ['nan', 'none', '']])\n",
        "\n",
        "def create_combined_text_v2(row):\n",
        "    \"\"\"Strategy v2: title + brand + color + bullets + description (prioritize metadata)\"\"\"\n",
        "    parts = [\n",
        "        str(row.get('product_title', '')),\n",
        "        str(row.get('product_brand', '')),\n",
        "        str(row.get('product_color', '')),\n",
        "        str(row.get('product_bullet_point', '')),\n",
        "        str(row.get('product_description', ''))\n",
        "    ]\n",
        "    return ' '.join([p for p in parts if p and p.lower() not in ['nan', 'none', '']])\n",
        "\n",
        "def create_combined_text_v3(row):\n",
        "    \"\"\"Strategy v3: title + bullets + brand + color + description (prioritize bullets)\"\"\"\n",
        "    parts = [\n",
        "        str(row.get('product_title', '')),\n",
        "        str(row.get('product_bullet_point', '')),\n",
        "        str(row.get('product_brand', '')),\n",
        "        str(row.get('product_color', '')),\n",
        "        str(row.get('product_description', ''))\n",
        "    ]\n",
        "    return ' '.join([p for p in parts if p and p.lower() not in ['nan', 'none', '']])\n",
        "\n",
        "# Create datasets with different combination strategies\n",
        "print(\"Creating datasets with different text combination strategies...\")\n",
        "\n",
        "df_baseline_v1 = df_baseline.copy()\n",
        "df_baseline_v1['combined_text_v1'] = df_baseline_v1.apply(create_combined_text_v1, axis=1)\n",
        "\n",
        "df_baseline_v2 = df_baseline.copy()\n",
        "df_baseline_v2['combined_text_v2'] = df_baseline_v2.apply(create_combined_text_v2, axis=1)\n",
        "\n",
        "df_baseline_v3 = df_baseline.copy()\n",
        "df_baseline_v3['combined_text_v3'] = df_baseline_v3.apply(create_combined_text_v3, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Embedding similarities between strategies:\n",
            "V1 vs V2: 0.9556\n",
            "V1 vs V3: 0.9548\n",
            "V2 vs V3: 0.9941\n",
            "\n",
            "Field ordering DOES affect embeddings significantly!\n",
            "We should evaluate all strategies systematically.\n",
            "\n",
            "Chosen strategy: evaluate_all\n"
          ]
        }
      ],
      "source": [
        "# Test to see if field ordering likely matters for embeddings\n",
        "## See if embeddings are much different between strategies first\n",
        "\n",
        "# Load a pre-trained model for quick testing\n",
        "from sentence_transformers import SentenceTransformer\n",
        "test_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings for each strategy\n",
        "embeddings_v1 = test_model.encode(df_baseline_v1['combined_text_v1'].tolist())\n",
        "embeddings_v2 = test_model.encode(df_baseline_v2['combined_text_v2'].tolist())\n",
        "embeddings_v3 = test_model.encode(df_baseline_v3['combined_text_v3'].tolist())\n",
        "\n",
        "# Calculate similarity between strategies\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sim_v1_v2 = np.mean([cosine_similarity([embeddings_v1[i]], [embeddings_v2[i]])[0][0] for i in range(len(df_baseline))])\n",
        "sim_v1_v3 = np.mean([cosine_similarity([embeddings_v1[i]], [embeddings_v3[i]])[0][0] for i in range(len(df_baseline))])\n",
        "sim_v2_v3 = np.mean([cosine_similarity([embeddings_v2[i]], [embeddings_v3[i]])[0][0] for i in range(len(df_baseline))])\n",
        "\n",
        "print(f\"\\nEmbedding similarities between strategies:\")\n",
        "print(f\"V1 vs V2: {sim_v1_v2:.4f}\")\n",
        "print(f\"V1 vs V3: {sim_v1_v3:.4f}\")\n",
        "print(f\"V2 vs V3: {sim_v2_v3:.4f}\")\n",
        "\n",
        "if min(sim_v1_v2, sim_v1_v3, sim_v2_v3) > 0.98:\n",
        "    print(\"\\nField ordering has MINIMAL impact on embeddings\")\n",
        "    print(\"We can proceed with V2 (metadata first) for conceptual clarity.\")\n",
        "    chosen_strategy = \"v2\"\n",
        "else:\n",
        "    print(\"\\nField ordering DOES affect embeddings significantly!\")\n",
        "    print(\"We should evaluate all strategies systematically.\")\n",
        "    chosen_strategy = \"evaluate_all\"\n",
        "\n",
        "print(f\"\\nChosen strategy: {chosen_strategy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEXT COMBINATION STRATEGY EVALUATION\n",
            "1. Evaluating Strategy V1 (Original Order)...\n",
            "Evaluating on 53 unique queries...\n",
            "2. Evaluating Strategy V2 (Metadata First)...\n",
            "Evaluating on 53 unique queries...\n",
            "3. Evaluating Strategy V3 (Bullets First)...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "STRATEGY COMPARISON\n",
            "========================================\n",
            "              Strategy  HITS@1  HITS@5    MRR\n",
            "0        V1 (Original)   0.962   0.981  0.972\n",
            "1  V2 (Metadata First)   0.943   0.981  0.962\n",
            "2   V3 (Bullets First)   0.943   0.981  0.962\n",
            "\n",
            "Winner: V1 (Original) (HITS@1: 0.962)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate text combination strategies\n",
        "## Simple search function for all strategies\n",
        "def simple_cosine_search(query, df, embeddings, model, top_k=10):\n",
        "    \"\"\"Simple cosine similarity search\"\"\"\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "    \n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            'product_id': df.iloc[idx]['product_id'],\n",
        "            'product_title': df.iloc[idx]['product_title'],\n",
        "            'score': similarities[idx]\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"TEXT COMBINATION STRATEGY EVALUATION\")\n",
        "\n",
        "print(\"1. Evaluating Strategy V1 (Original Order)...\")\n",
        "v1_metrics = evaluate_search_system(df_baseline_v1, simple_cosine_search, embeddings_v1, test_model)\n",
        "\n",
        "print(\"2. Evaluating Strategy V2 (Metadata First)...\")\n",
        "v2_metrics = evaluate_search_system(df_baseline_v2, simple_cosine_search, embeddings_v2, test_model)\n",
        "\n",
        "print(\"3. Evaluating Strategy V3 (Bullets First)...\")\n",
        "v3_metrics = evaluate_search_system(df_baseline_v3, simple_cosine_search, embeddings_v3, test_model)\n",
        "\n",
        "# Compare results\n",
        "strategy_comparison = pd.DataFrame({\n",
        "    'Strategy': ['V1 (Original)', 'V2 (Metadata First)', 'V3 (Bullets First)'],\n",
        "    'HITS@1': [v1_metrics['hits_at_1'], v2_metrics['hits_at_1'], v3_metrics['hits_at_1']],\n",
        "    'HITS@5': [v1_metrics['hits_at_5'], v2_metrics['hits_at_5'], v3_metrics['hits_at_5']],\n",
        "    'MRR': [v1_metrics['mrr'], v2_metrics['mrr'], v3_metrics['mrr']]\n",
        "})\n",
        "\n",
        "print(\"\\nSTRATEGY COMPARISON\")\n",
        "print(\"=\" * 40)\n",
        "print(strategy_comparison.round(3))\n",
        "\n",
        "# Find winner\n",
        "best_idx = strategy_comparison['HITS@1'].idxmax()\n",
        "best_strategy = strategy_comparison.loc[best_idx, 'Strategy']\n",
        "print(f\"\\nWinner: {best_strategy} (HITS@1: {strategy_comparison.loc[best_idx, 'HITS@1']:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Original text combining in current format performed best with marginally higher HITS@1 and MRR\n",
        "  * Different imputation strategies could affect this in the future but will proceed with original text combination "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save embeddings for potential reuse\n",
        "np.save(\"../src/embeddings/combining/strategy_v1_embeddings.npy\", embeddings_v1)\n",
        "np.save(\"../src/embeddings/combining/strategy_v2_embeddings.npy\", embeddings_v2)\n",
        "np.save(\"../src/embeddings/combining/strategy_v3_embeddings.npy\", embeddings_v3)\n",
        "\n",
        "# Save model name\n",
        "with open(\"../src/embeddings/combining/model_name.txt\", \"w\") as f:\n",
        "    f.write(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prefer to build complete vector database from scratch with current v_1 strategy\n",
        "## rather than using pre-computed embeddings\n",
        "def create_embeddings_and_vector_db(df, text_column, model_name=\"all-MiniLM-L6-v2\", db_path_suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Complete pipeline: raw text -> embeddings -> vector database\n",
        "    This demonstrates the full workflow a production system would use\n",
        "    \"\"\"\n",
        "    print(f\"Creating embeddings for {len(df)} products using {model_name}...\")\n",
        "    \n",
        "    # Step 1: Load transformer model\n",
        "    model = SentenceTransformer(model_name)\n",
        "    \n",
        "    # Step 2: Create embeddings from text\n",
        "    texts = df[text_column].tolist()\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    print(f\"Created embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    # Step 3: Create LanceDB vector database\n",
        "    db_path = f\"../src/vector_databases/lancedb_hybrid{db_path_suffix}\"\n",
        "    os.makedirs(db_path, exist_ok=True)\n",
        "    db = lancedb.connect(db_path)\n",
        "    \n",
        "    # Prepare data for LanceDB\n",
        "    data = []\n",
        "    for idx, (_, row) in enumerate(df.iterrows()):\n",
        "        data.append({\n",
        "            \"vector\": embeddings[idx].tolist(),\n",
        "            \"product_id\": row['product_id'],\n",
        "            \"query\": row['query'],\n",
        "            \"product_title\": row['product_title'],\n",
        "            \"product_description\": row.get('product_description', ''),\n",
        "            \"product_bullet_point\": row.get('product_bullet_point', ''),\n",
        "            \"product_brand\": row.get('product_brand', ''),\n",
        "            \"product_color\": row.get('product_color', ''),\n",
        "            \"combined_text\": row[text_column]\n",
        "        })\n",
        "    \n",
        "    # Create table (drop if exists)\n",
        "    table_name = \"products\"\n",
        "    if table_name in db.table_names():\n",
        "        db.drop_table(table_name)\n",
        "    \n",
        "    table = db.create_table(table_name, data)\n",
        "    \n",
        "    print(f\"Created LanceDB table: {len(data)} vectors, {embeddings.shape[1]} dimensions\")\n",
        "    print(f\"Database path: {db_path}\")\n",
        "    \n",
        "    return model, embeddings, db, table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating embeddings for 519 products using all-MiniLM-L6-v2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "099c2f1b9e5142768263066138322eb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created embeddings shape: (519, 384)\n",
            "Created LanceDB table: 519 vectors, 384 dimensions\n",
            "Database path: ../src/vector_databases/lancedb_hybrid_v1\n"
          ]
        }
      ],
      "source": [
        "# Strategy V2 (metadata first) - this addresses our truncation concerns\n",
        "model_v1, embeddings_v1, db_v1, table_v1 = create_embeddings_and_vector_db(\n",
        "    df_baseline_v1, 'combined_text_v1', db_path_suffix=\"_v1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hybrid search implementation (Keyword + Semantic)\n",
        "def calculate_keyword_score(query, product_text, keywords_dict):\n",
        "    \"\"\"\n",
        "    Calculate keyword matching score using frequency-weighted word matching\n",
        "    keywords_dict contains word -> frequency counts\n",
        "    \"\"\"\n",
        "    query_words = [word.lower().strip() for word in query.split()]\n",
        "    product_text_lower = product_text.lower()\n",
        "    \n",
        "    if not query_words:\n",
        "        return 0.0\n",
        "    \n",
        "    total_weight = 0\n",
        "    matched_weight = 0\n",
        "    \n",
        "    for word in query_words:\n",
        "        if word in keywords_dict:\n",
        "            # Use inverse frequency as weight (rare words are more important)\n",
        "            frequency = keywords_dict[word]\n",
        "            weight = 1.0 / (1.0 + frequency)  # Inverse frequency weighting\n",
        "            total_weight += weight\n",
        "            \n",
        "            # Check if word appears in product text\n",
        "            if word in product_text_lower:\n",
        "                matched_weight += weight\n",
        "    \n",
        "    if total_weight == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Return weighted fraction of matched keywords\n",
        "    return matched_weight / total_weight\n",
        "\n",
        "def hybrid_search(query, df, table, model, keywords_dict, alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=10):\n",
        "    \"\"\"\n",
        "    Hybrid search combining semantic similarity and keyword matching\n",
        "    alpha: weight for semantic score\n",
        "    beta: weight for keyword score\n",
        "    \"\"\"\n",
        "    # step 1: get semantic search results\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    # Using top_k * 2 to allow for reranking - products low in semantic might rank high in hybrid\n",
        "    semantic_results = table.search(query_embedding).distance_type(\"cosine\").limit(top_k * 2).to_pandas()\n",
        "    \n",
        "    # step 2: calc hybrid scores\n",
        "    hybrid_scores = []\n",
        "    for idx, row in semantic_results.iterrows():\n",
        "        semantic_score = 1 - row['_distance']  # Convert distance to similarity\n",
        "        keyword_score = calculate_keyword_score(query, row['combined_text'], keywords_dict)\n",
        "        \n",
        "        # combine scores - only apply keyword boost if above threshold\n",
        "        if keyword_score >= keyword_threshold:\n",
        "            hybrid_score = alpha * semantic_score + beta * keyword_score\n",
        "        else:\n",
        "            hybrid_score = semantic_score  # Pure semantic if keywords don't meet threshold\n",
        "        \n",
        "        hybrid_scores.append({\n",
        "            'semantic_score': semantic_score,\n",
        "            'keyword_score': keyword_score,\n",
        "            'hybrid_score': hybrid_score,\n",
        "            'product_id': row['product_id'],\n",
        "            'product_title': row['product_title'],\n",
        "            'combined_text': row['combined_text']\n",
        "        })\n",
        "    \n",
        "    # step3: sort by hybrid score and return top-k\n",
        "    hybrid_scores.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
        "    top_results = hybrid_scores[:top_k]\n",
        "    \n",
        "    # Converting back to df format for eval\n",
        "    result_df = pd.DataFrame(top_results)\n",
        "    result_df['score'] = result_df['hybrid_score']\n",
        "    \n",
        "    return result_df\n",
        "\n",
        "def pure_semantic_search(query, df, table, model, top_k=10):\n",
        "    \"\"\"Pure semantic search for comparison\"\"\"\n",
        "    query_embedding = model.encode([query])[0]\n",
        "    results = table.search(query_embedding).distance_type(\"cosine\").limit(top_k).to_pandas()\n",
        "    results['score'] = 1 - results['_distance']\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing hybrid search with query: 'coffee maker'\n",
            "\n",
            "----  Pure Semantic Search  -----\n",
            "Score: 0.613 | Cuisinart DCC-3200P1 Perfectemp Coffee Maker, 14 Cup Progammable with ...\n",
            "Score: 0.582 | Presto 02835 MyJo Single Cup Coffee Maker, Black...\n",
            "Score: 0.567 | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, InstaCoffee Br...\n",
            "Score: 0.551 | Elite Gourmet EHC111A Maxi-Matic Personal 14oz Single-Serve Compact Co...\n",
            "Score: 0.530 | Single Serve K Cup Coffee Maker for K-Cup Pods and Ground Coffee, Comp...\n",
            "\n",
            "----  Hybrid Search (70/30 semantic/keyword split)  -----\n",
            "Hybrid: 0.729 (S:0.613, K:1.000) | Cuisinart DCC-3200P1 Perfectemp Coffee Maker, 14 Cup Progamm...\n",
            "Hybrid: 0.707 (S:0.582, K:1.000) | Presto 02835 MyJo Single Cup Coffee Maker, Black...\n",
            "Hybrid: 0.697 (S:0.567, K:1.000) | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, Inst...\n",
            "Hybrid: 0.686 (S:0.551, K:1.000) | Elite Gourmet EHC111A Maxi-Matic Personal 14oz Single-Serve ...\n",
            "Hybrid: 0.671 (S:0.530, K:1.000) | Single Serve K Cup Coffee Maker for K-Cup Pods and Ground Co...\n"
          ]
        }
      ],
      "source": [
        "# Test hybrid search performance\n",
        "test_query = \"coffee maker\"\n",
        "print(f\"Testing hybrid search with query: '{test_query}'\")\n",
        "\n",
        "# Pure semantic results\n",
        "print(\"\\n----  Pure Semantic Search  -----\")\n",
        "semantic_results = pure_semantic_search(test_query, df_baseline_v1, table_v1, model_v1, top_k=5)\n",
        "for idx, row in semantic_results.iterrows():\n",
        "    print(f\"Score: {row['score']:.3f} | {row['product_title'][:70]}...\")\n",
        "\n",
        "# Hybrid search results  \n",
        "print(\"\\n----  Hybrid Search (70/30 semantic/keyword split)  -----\")\n",
        "hybrid_results = hybrid_search(test_query, df_baseline_v1, table_v1, model_v1, light_keywords, \n",
        "                               alpha=0.7, beta=0.3, top_k=5)\n",
        "for idx, row in hybrid_results.iterrows():\n",
        "    print(f\"Hybrid: {row['hybrid_score']:.3f} (S:{row['semantic_score']:.3f}, K:{row['keyword_score']:.3f}) | {row['product_title'][:60]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "HYBRID SEARCH EVALUATION (V1 Strategy)\n",
            "============================================================\n",
            "Using V1 strategy (Original Order) - the winning text combination approach\n",
            "Database: 519 products, 519 vectors\n",
            "\n",
            "1. Evaluating pure semantic search (V1 strategy)...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "--- Pure Semantic Search Performance ---\n",
            "HITS@1:  0.962\n",
            "HITS@5:  0.981\n",
            "HITS@10: 0.981\n",
            "MRR:     0.972\n",
            "\n",
            "2. Evaluating hybrid search (light keywords)...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "--- Hybrid Search (Light Keywords) Performance ---\n",
            "HITS@1:  0.981\n",
            "HITS@5:  0.981\n",
            "HITS@10: 0.981\n",
            "MRR:     0.981\n",
            "\n",
            "3. Evaluating hybrid search (moderate keywords)...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "--- Hybrid Search (Moderate Keywords) Performance ---\n",
            "HITS@1:  0.981\n",
            "HITS@5:  0.981\n",
            "HITS@10: 0.981\n",
            "MRR:     0.981\n",
            "\n",
            "============================================================\n",
            "HYBRID SEARCH COMPARISON RESULTS\n",
            "============================================================\n",
            "              Method  HITS@1  HITS@5  HITS@10    MRR\n",
            "0      Pure Semantic   0.962   0.981    0.981  0.972\n",
            "1     Hybrid (Light)   0.981   0.981    0.981  0.981\n",
            "2  Hybrid (Moderate)   0.981   0.981    0.981  0.981\n",
            "\n",
            "--- Key Insights ---\n",
            "Best HITS@1: Hybrid (Light) (0.981)\n",
            "Best MRR: Hybrid (Light) (0.981)\n",
            "\n",
            "Hybrid improvements over pure semantic:\n",
            "Light keywords:    +0.019 HITS@1\n",
            "Moderate keywords: +0.019 HITS@1\n",
            "Light vs Moderate: +0.000 HITS@1 difference\n"
          ]
        }
      ],
      "source": [
        "# Evaluate hybrid search approaches\n",
        "def print_evaluation_results(results, method_name):\n",
        "    \"\"\"Pretty print evaluation results\"\"\"\n",
        "    print(f\"\\n--- {method_name} Performance ---\")\n",
        "    print(f\"HITS@1:  {results['hits_at_1']:.3f}\")\n",
        "    print(f\"HITS@5:  {results['hits_at_5']:.3f}\")\n",
        "    print(f\"HITS@10: {results['hits_at_10']:.3f}\")\n",
        "    print(f\"MRR:     {results['mrr']:.3f}\")\n",
        "\n",
        "# Create wrapper functions for evaluation compatibility with V1 strategy\n",
        "def eval_semantic_search(query, df, *args, top_k=10):\n",
        "    \"\"\"Wrapper for semantic search evaluation\"\"\"\n",
        "    table, model = args\n",
        "    return pure_semantic_search(query, df, table, model, top_k)\n",
        "\n",
        "def eval_hybrid_search_light(query, df, *args, top_k=10):\n",
        "    \"\"\"Wrapper for hybrid search with light keywords\"\"\"\n",
        "    table, model = args\n",
        "    return hybrid_search(query, df, table, model, light_keywords, alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=top_k)\n",
        "\n",
        "def eval_hybrid_search_moderate(query, df, *args, top_k=10):\n",
        "    \"\"\"Wrapper for hybrid search with moderate keywords\"\"\"\n",
        "    table, model = args\n",
        "    return hybrid_search(query, df, table, model, moderate_keywords, alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=top_k)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HYBRID SEARCH EVALUATION (V1 Strategy)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use the existing V1 LanceDB setup (winner from text combination evaluation)\n",
        "print(\"Using V1 strategy (Original Order) - the winning text combination approach\")\n",
        "print(f\"Database: {len(df_baseline_v1)} products, {table_v1.count_rows()} vectors\")\n",
        "\n",
        "# Evaluate pure semantic search using V1\n",
        "print(\"\\n1. Evaluating pure semantic search (V1 strategy)...\")\n",
        "semantic_metrics = evaluate_search_system(\n",
        "    df_baseline_v1, eval_semantic_search, table_v1, model_v1\n",
        ")\n",
        "print_evaluation_results(semantic_metrics, \"Pure Semantic Search\")\n",
        "\n",
        "# Evaluate hybrid search with light keywords\n",
        "print(\"\\n2. Evaluating hybrid search (light keywords)...\")\n",
        "hybrid_light_metrics = evaluate_search_system(\n",
        "    df_baseline_v1, eval_hybrid_search_light, table_v1, model_v1\n",
        ")\n",
        "print_evaluation_results(hybrid_light_metrics, \"Hybrid Search (Light Keywords)\")\n",
        "\n",
        "# Evaluate hybrid search with moderate keywords  \n",
        "print(\"\\n3. Evaluating hybrid search (moderate keywords)...\")\n",
        "hybrid_moderate_metrics = evaluate_search_system(\n",
        "    df_baseline_v1, eval_hybrid_search_moderate, table_v1, model_v1\n",
        ")\n",
        "print_evaluation_results(hybrid_moderate_metrics, \"Hybrid Search (Moderate Keywords)\")\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HYBRID SEARCH COMPARISON RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Method': ['Pure Semantic', 'Hybrid (Light)', 'Hybrid (Moderate)'],\n",
        "    'HITS@1': [semantic_metrics['hits_at_1'], hybrid_light_metrics['hits_at_1'], hybrid_moderate_metrics['hits_at_1']],\n",
        "    'HITS@5': [semantic_metrics['hits_at_5'], hybrid_light_metrics['hits_at_5'], hybrid_moderate_metrics['hits_at_5']],\n",
        "    'HITS@10': [semantic_metrics['hits_at_10'], hybrid_light_metrics['hits_at_10'], hybrid_moderate_metrics['hits_at_10']],\n",
        "    'MRR': [semantic_metrics['mrr'], hybrid_light_metrics['mrr'], hybrid_moderate_metrics['mrr']]\n",
        "})\n",
        "\n",
        "print(comparison_df.round(3))\n",
        "\n",
        "# Performance insights\n",
        "print(\"\\n--- Key Insights ---\")\n",
        "best_hits1_idx = comparison_df['HITS@1'].idxmax()\n",
        "best_method = comparison_df.loc[best_hits1_idx, 'Method']\n",
        "best_score = comparison_df.loc[best_hits1_idx, 'HITS@1']\n",
        "print(f\"Best HITS@1: {best_method} ({best_score:.3f})\")\n",
        "\n",
        "best_mrr_idx = comparison_df['MRR'].idxmax()\n",
        "best_mrr_method = comparison_df.loc[best_mrr_idx, 'Method'] \n",
        "best_mrr_score = comparison_df.loc[best_mrr_idx, 'MRR']\n",
        "print(f\"Best MRR: {best_mrr_method} ({best_mrr_score:.3f})\")\n",
        "\n",
        "# Calculate improvements\n",
        "light_improvement = hybrid_light_metrics['hits_at_1'] - semantic_metrics['hits_at_1']\n",
        "moderate_improvement = hybrid_moderate_metrics['hits_at_1'] - semantic_metrics['hits_at_1']\n",
        "print(f\"\\nHybrid improvements over pure semantic:\")\n",
        "print(f\"Light keywords:    {light_improvement:+.3f} HITS@1\")\n",
        "print(f\"Moderate keywords: {moderate_improvement:+.3f} HITS@1\")\n",
        "\n",
        "# Keyword comparison\n",
        "keyword_diff = hybrid_light_metrics['hits_at_1'] - hybrid_moderate_metrics['hits_at_1']\n",
        "print(f\"Light vs Moderate: {keyword_diff:+.3f} HITS@1 difference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Hybrid (light_keywords + semantic) approach slightly better than pure semantic search\n",
        "  * Note moderate was just lemmatizing light keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "BASELINE vs IMPUTED DATA COMPARISON (V1 Strategy)\n",
            "============================================================\n",
            "Creating vector database for imputed dataset using V1 strategy...\n",
            "Creating embeddings for 519 products using all-MiniLM-L6-v2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53b790306a1a441083ff0f0f89de7d33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created embeddings shape: (519, 384)\n",
            "Created LanceDB table: 519 vectors, 384 dimensions\n",
            "Database path: ../src/vector_databases/lancedb_hybrid_imputed_v1\n",
            "Imputed database: 519 products, 519 vectors\n",
            "\n",
            "1. Evaluating semantic search on imputed data...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "--- Semantic Search (Imputed Data) Performance ---\n",
            "HITS@1:  0.962\n",
            "HITS@5:  0.981\n",
            "HITS@10: 0.981\n",
            "MRR:     0.972\n",
            "\n",
            "2. Evaluating hybrid search on imputed data...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "--- Hybrid Search (Imputed Data) Performance ---\n",
            "HITS@1:  0.981\n",
            "HITS@5:  0.981\n",
            "HITS@10: 0.981\n",
            "MRR:     0.981\n",
            "\n",
            "============================================================\n",
            "BASELINE vs IMPUTED PERFORMANCE SUMMARY\n",
            "============================================================\n",
            "         Dataset  Semantic_HITS@1  Semantic_MRR  Hybrid_HITS@1  Hybrid_MRR\n",
            "0  Baseline (V1)            0.962         0.972          0.981       0.981\n",
            "1   Imputed (V1)            0.962         0.972          0.981       0.981\n",
            "\n",
            "--- Imputation Impact ---\n",
            "Semantic search:\n",
            "  HITS@1: +0.000\n",
            "  MRR:    +0.000\n",
            "Hybrid search:\n",
            "  HITS@1: +0.000\n",
            "  MRR:    +0.000\n",
            "\n",
            "Imputation does not improve search performance\n",
            "Recommendation: Use Baseline dataset for production\n"
          ]
        }
      ],
      "source": [
        "# Prepare imputed dataset with original text combination strategy\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE vs IMPUTED DATA COMPARISON (V1 Strategy)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_imputed_v1 = df_imputed.copy()\n",
        "df_imputed_v1['combined_text_v1'] = df_imputed_v1.apply(create_combined_text_v1, axis=1)\n",
        "\n",
        "print(\"Creating vector database for imputed dataset using V1 strategy...\")\n",
        "model_imputed, embeddings_imputed, db_imputed, table_imputed = create_embeddings_and_vector_db(\n",
        "    df_imputed_v1, 'combined_text_v1', db_path_suffix=\"_imputed_v1\"\n",
        ")\n",
        "\n",
        "print(f\"Imputed database: {len(df_imputed_v1)} products, {table_imputed.count_rows()} vectors\")\n",
        "\n",
        "# Evaluate semantic search on imputed data\n",
        "print(\"\\n1. Evaluating semantic search on imputed data...\")\n",
        "semantic_imputed_metrics = evaluate_search_system(\n",
        "    df_imputed_v1, eval_semantic_search, table_imputed, model_imputed\n",
        ")\n",
        "print_evaluation_results(semantic_imputed_metrics, \"Semantic Search (Imputed Data)\")\n",
        "\n",
        "# Evaluate hybrid search on imputed data  \n",
        "print(\"\\n2. Evaluating hybrid search on imputed data...\")\n",
        "def eval_hybrid_imputed(query, df, *args, top_k=10):\n",
        "    table, model = args\n",
        "    return hybrid_search(query, df, table, model, light_keywords, alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=top_k)\n",
        "\n",
        "hybrid_imputed_metrics = evaluate_search_system(\n",
        "    df_imputed_v1, eval_hybrid_imputed, table_imputed, model_imputed\n",
        ")\n",
        "print_evaluation_results(hybrid_imputed_metrics, \"Hybrid Search (Imputed Data)\")\n",
        "\n",
        "# Compare baseline vs imputed performance\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BASELINE vs IMPUTED PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "imputed_comparison = pd.DataFrame({\n",
        "    'Dataset': ['Baseline (V1)', 'Imputed (V1)'],\n",
        "    'Semantic_HITS@1': [semantic_metrics['hits_at_1'], semantic_imputed_metrics['hits_at_1']],\n",
        "    'Semantic_MRR': [semantic_metrics['mrr'], semantic_imputed_metrics['mrr']],\n",
        "    'Hybrid_HITS@1': [hybrid_light_metrics['hits_at_1'], hybrid_imputed_metrics['hits_at_1']],\n",
        "    'Hybrid_MRR': [hybrid_light_metrics['mrr'], hybrid_imputed_metrics['mrr']]\n",
        "})\n",
        "\n",
        "print(imputed_comparison.round(3))\n",
        "\n",
        "# Calculate and display potential improvements from imputation\n",
        "semantic_improvement = semantic_imputed_metrics['hits_at_1'] - semantic_metrics['hits_at_1']\n",
        "hybrid_improvement = hybrid_imputed_metrics['hits_at_1'] - hybrid_light_metrics['hits_at_1']\n",
        "semantic_mrr_improvement = semantic_imputed_metrics['mrr'] - semantic_metrics['mrr']\n",
        "hybrid_mrr_improvement = hybrid_imputed_metrics['mrr'] - hybrid_light_metrics['mrr']\n",
        "\n",
        "print(f\"\\n--- Imputation Impact ---\")\n",
        "print(f\"Semantic search:\")\n",
        "print(f\"  HITS@1: {semantic_improvement:+.3f}\")\n",
        "print(f\"  MRR:    {semantic_mrr_improvement:+.3f}\")\n",
        "print(f\"Hybrid search:\")\n",
        "print(f\"  HITS@1: {hybrid_improvement:+.3f}\")\n",
        "print(f\"  MRR:    {hybrid_mrr_improvement:+.3f}\")\n",
        "\n",
        "# Determine if imputation helps\n",
        "if semantic_improvement > 0 or hybrid_improvement > 0:\n",
        "    print(f\"\\nImputation IMPROVES search performance\")\n",
        "    best_dataset = \"Imputed\"\n",
        "else:\n",
        "    print(f\"\\nImputation does not improve search performance\")\n",
        "    best_dataset = \"Baseline\"\n",
        "\n",
        "print(f\"Recommendation: Use {best_dataset} dataset for production\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Continue with baseline data and original combined text utilizing hybrid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate different alpha/beta weight combinations for hybrid search\n",
        "def evaluate_hybrid_weights(df, table, model, keywords_dict, alpha_values):\n",
        "    \"\"\"Test different alpha/beta weight combinations for hybrid search\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for alpha in alpha_values:\n",
        "        beta = 1 - alpha\n",
        "        print(f\"Testing alpha={alpha:.1f}, beta={beta:.1f}...\")\n",
        "        \n",
        "        def eval_hybrid_weighted(query, df, *args, top_k=10):\n",
        "            table, model = args\n",
        "            return hybrid_search(query, df, table, model, keywords_dict, alpha, beta, keyword_threshold=0.1, top_k=top_k)\n",
        "        \n",
        "        metrics = evaluate_search_system(df, eval_hybrid_weighted, table, model, k_values=[1, 5])\n",
        "        metrics['alpha'] = alpha\n",
        "        metrics['beta'] = beta\n",
        "        results.append(metrics)\n",
        "    \n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing hybrid search weights...\n",
            "Testing alpha=0.5, beta=0.5...\n",
            "Evaluating on 53 unique queries...\n",
            "Testing alpha=0.6, beta=0.4...\n",
            "Evaluating on 53 unique queries...\n",
            "Testing alpha=0.7, beta=0.3...\n",
            "Evaluating on 53 unique queries...\n",
            "Testing alpha=0.8, beta=0.2...\n",
            "Evaluating on 53 unique queries...\n",
            "Testing alpha=0.9, beta=0.1...\n",
            "Evaluating on 53 unique queries...\n",
            "Testing alpha=1.0, beta=0.0...\n",
            "Evaluating on 53 unique queries...\n",
            "\n",
            "----  Weight Optimization Results  ------\n",
            "   alpha  beta  hits_at_1  hits_at_5    mrr\n",
            "0    0.5   0.5      0.981      0.981  0.981\n",
            "1    0.6   0.4      0.981      0.981  0.981\n",
            "2    0.7   0.3      0.981      0.981  0.981\n",
            "3    0.8   0.2      0.981      0.981  0.981\n",
            "4    0.9   0.1      0.981      0.981  0.981\n",
            "5    1.0   0.0      0.962      0.981  0.972\n",
            "\n",
            "Optimal weights: alpha=0.5, beta=0.5\n",
            "Best HITS@1: 0.981\n",
            "Improvement over pure semantic: +0.019 HITS@1\n"
          ]
        }
      ],
      "source": [
        "# Test different weight combinations\n",
        "print(\"Optimizing hybrid search weights...\")\n",
        "alpha_values = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # 1.0 = pure semantic\n",
        "\n",
        "weight_results = evaluate_hybrid_weights(df_baseline_v1, table_v1, model_v1, light_keywords, alpha_values)\n",
        "\n",
        "print(\"\\n----  Weight Optimization Results  ------\")\n",
        "print(weight_results[['alpha', 'beta', 'hits_at_1', 'hits_at_5', 'mrr']].round(3))\n",
        "\n",
        "# Find optimal weights\n",
        "best_idx = weight_results['hits_at_1'].idxmax()\n",
        "optimal_alpha = weight_results.loc[best_idx, 'alpha']\n",
        "optimal_beta = weight_results.loc[best_idx, 'beta']\n",
        "best_hits1 = weight_results.loc[best_idx, 'hits_at_1']\n",
        "\n",
        "print(f\"\\nOptimal weights: alpha={optimal_alpha:.1f}, beta={optimal_beta:.1f}\")\n",
        "print(f\"Best HITS@1: {best_hits1:.3f}\")\n",
        "\n",
        "# Check if hybrid is better than pure semantic\n",
        "pure_semantic_hits1 = weight_results[weight_results['alpha'] == 1.0]['hits_at_1'].iloc[0]\n",
        "improvement = best_hits1 - pure_semantic_hits1\n",
        "print(f\"Improvement over pure semantic: {improvement:+.3f} HITS@1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Not any improvement by varying alpha/beta combos\n",
        "  * Will keep at 70/30 split for strong semantic foundation with keyword enhancement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Secondary Ranking Implementation\n",
        "\n",
        "As specified in the assessment, let's implement business logic re-ranking on top of our search results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look into secondary ranking on top of hybrid search\n",
        "def apply_secondary_ranking(search_results, boost_brands=None, brand_boost=0.1):\n",
        "    \"\"\"\n",
        "    Apply business logic secondary ranking to search results\n",
        "    To demonstrate practical production considerations beyond pure relevance\n",
        "    \"\"\"\n",
        "    results = search_results.copy()\n",
        "    \n",
        "    # Business rule 1: Brand preference boost\n",
        "    if boost_brands:\n",
        "        for brand in boost_brands:\n",
        "            brand_mask = results['product_title'].str.contains(brand, case=False, na=False)\n",
        "            results.loc[brand_mask, 'score'] += brand_boost\n",
        "    \n",
        "    # Business rule 2: Exact title match boost\n",
        "    query_words = set(results.iloc[0].get('query', '').lower().split()) if len(results) > 0 else set()\n",
        "    for idx, row in results.iterrows():\n",
        "        title_words = set(str(row.get('product_title', '')).lower().split())\n",
        "        overlap = len(query_words.intersection(title_words))\n",
        "        if overlap >= 2:  # At least 2 words match\n",
        "            results.loc[idx, 'score'] += 0.05 * overlap\n",
        "    \n",
        "    # Re-sort by updated scores\n",
        "    results = results.sort_values('score', ascending=False).reset_index(drop=True)\n",
        "    return results\n",
        "\n",
        "def hybrid_search_with_ranking(query, df, table, model, keywords_dict, \n",
        "                               boost_brands=None, alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=10):\n",
        "    \"\"\"Hybrid search with secondary ranking\"\"\"\n",
        "    # Get initial hybrid results\n",
        "    initial_results = hybrid_search(query, df, table, model, keywords_dict, alpha, beta, keyword_threshold=0.1, top_k=top_k * 2)\n",
        "    # Apply secondary ranking\n",
        "    ranked_results = apply_secondary_ranking(initial_results, boost_brands)\n",
        "     # Return top-k after ranking\n",
        "    return ranked_results.head(top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing secondary ranking with brand boost...\n",
            "\n",
            "Query: 'coffee maker'\n",
            "\n",
            "-----   Without Secondary Ranking   -------\n",
            "Score: 0.729 | Cuisinart DCC-3200P1 Perfectemp Coffee Maker, 14 Cup Progammable with ...\n",
            "Score: 0.707 | Presto 02835 MyJo Single Cup Coffee Maker, Black...\n",
            "Score: 0.697 | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, InstaCoffee Br...\n",
            "Score: 0.686 | Elite Gourmet EHC111A Maxi-Matic Personal 14oz Single-Serve Compact Co...\n",
            "Score: 0.671 | Single Serve K Cup Coffee Maker for K-Cup Pods and Ground Coffee, Comp...\n",
            "\n",
            "-------  With Secondary Ranking (Hamilton Beach boost)   ------\n",
            "Score: 0.763 [BOOSTED] | Hamilton Beach (47950) Coffee Maker with 12 Cup Capacity & Internal St...\n",
            "Score: 0.729 | Cuisinart DCC-3200P1 Perfectemp Coffee Maker, 14 Cup Progammable with ...\n",
            "Score: 0.707 | Presto 02835 MyJo Single Cup Coffee Maker, Black...\n",
            "Score: 0.697 | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, InstaCoffee Br...\n",
            "Score: 0.686 | Elite Gourmet EHC111A Maxi-Matic Personal 14oz Single-Serve Compact Co...\n"
          ]
        }
      ],
      "source": [
        "# Test secondary ranking\n",
        "print(\"Testing secondary ranking with brand boost...\")\n",
        "\n",
        "test_query = \"coffee maker\"\n",
        "print(f\"\\nQuery: '{test_query}'\")\n",
        "\n",
        "# Without secondary ranking - just show pure hybrid results\n",
        "print(\"\\n-----   Without Secondary Ranking   -------\")\n",
        "base_results = hybrid_search(test_query, df_baseline_v1, table_v1, model_v1, light_keywords, \n",
        "                             alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=5)\n",
        "for idx, row in base_results.iterrows():\n",
        "    print(f\"Score: {row['hybrid_score']:.3f} | {row['product_title'][:70]}...\")\n",
        "\n",
        "# With secondary ranking - show Hamilton Beach boost effect\n",
        "print(\"\\n-------  With Secondary Ranking (Hamilton Beach boost)   ------\")\n",
        "ranked_results = hybrid_search_with_ranking(test_query, df_baseline_v1, table_v1, model_v1, light_keywords,\n",
        "                    boost_brands=['Hamilton Beach'], alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=5)\n",
        "for idx, row in ranked_results.iterrows():\n",
        "    is_hamilton_beach = 'Hamilton Beach' in str(row.get('product_title', ''))\n",
        "    boost_indicator = \" [BOOSTED]\" if is_hamilton_beach else \"\"\n",
        "    print(f\"Score: {row['score']:.3f}{boost_indicator} | {row['product_title'][:70]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing secondary ranking with brand boost...\n",
            "\n",
            "Query: 'coffee maker'\n",
            "\n",
            "-----   Without Secondary Ranking   -------\n",
            "Score: 0.729 | Cuisinart DCC-3200P1 Perfectemp Coffee Maker, 14 Cup Progammable with ...\n",
            "Score: 0.707 | Presto 02835 MyJo Single Cup Coffee Maker, Black...\n",
            "Score: 0.697 | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, InstaCoffee Br...\n",
            "Score: 0.686 | Elite Gourmet EHC111A Maxi-Matic Personal 14oz Single-Serve Compact Co...\n",
            "Score: 0.671 | Single Serve K Cup Coffee Maker for K-Cup Pods and Ground Coffee, Comp...\n",
            "\n",
            "-------  With Secondary Ranking (CHEFMAN boost)   ------\n",
            "Score: 0.797 [BOOSTED] | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, InstaCoffee Br...\n",
            "Score: 0.729 | Cuisinart DCC-3200P1 Perfectemp Coffee Maker, 14 Cup Progammable with ...\n",
            "Score: 0.707 | Presto 02835 MyJo Single Cup Coffee Maker, Black...\n",
            "Score: 0.686 | Elite Gourmet EHC111A Maxi-Matic Personal 14oz Single-Serve Compact Co...\n",
            "Score: 0.671 | Single Serve K Cup Coffee Maker for K-Cup Pods and Ground Coffee, Comp...\n"
          ]
        }
      ],
      "source": [
        "# Test secondary ranking\n",
        "print(\"Testing secondary ranking with brand boost...\")\n",
        "\n",
        "test_query = \"coffee maker\"\n",
        "print(f\"\\nQuery: '{test_query}'\")\n",
        "\n",
        "# Without secondary ranking - just show pure hybrid results\n",
        "print(\"\\n-----   Without Secondary Ranking   -------\")\n",
        "base_results = hybrid_search(test_query, df_baseline_v1, table_v1, model_v1, light_keywords, \n",
        "                             alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=5)\n",
        "for idx, row in base_results.iterrows():\n",
        "    print(f\"Score: {row['hybrid_score']:.3f} | {row['product_title'][:70]}...\")\n",
        "\n",
        "# With secondary ranking - show CHEFMAN boost effect (able to see it from un-boosted results)\n",
        "print(\"\\n-------  With Secondary Ranking (CHEFMAN boost)   ------\")\n",
        "ranked_results = hybrid_search_with_ranking(test_query, df_baseline_v1, table_v1, model_v1, light_keywords,\n",
        "                    boost_brands=['CHEFMAN'], alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=5)\n",
        "for idx, row in ranked_results.iterrows():\n",
        "    is_chefman = 'CHEFMAN' in str(row.get('product_title', ''))\n",
        "    boost_indicator = \" [BOOSTED]\" if is_chefman else \"\"\n",
        "    print(f\"Score: {row['score']:.3f}{boost_indicator} | {row['product_title'][:70]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing secondary ranking with query/title overlap boost...\n",
            "\n",
            "Query: 'single serve K Cup coffee maker for K-Cup pods'\n",
            "\n",
            "-------  With Secondary Ranking (query overlap)   ------\n",
            "Score: 0.807 | Keurig K-Mini Coffee Maker, Single Serve K-Cup Pod Coffee Brewer, 6 to...\n",
            "Score: 0.788 | Single Serve K Cup Coffee Maker for K-Cup Pods and Ground Coffee, Comp...\n",
            "Score: 0.787 | Keurig K-Mini Plus Coffee Maker, Single Serve K-Cup Pod Coffee Brewer,...\n",
            "Score: 0.785 | Keurig MAIN-85544 Compact Single-Serve K-Cup Pod Coffee Maker, Black, ...\n",
            "Score: 0.743 | CHEFMAN Single Serve One Cup Coffee Maker, up to 14 Oz, InstaCoffee Br...\n"
          ]
        }
      ],
      "source": [
        "# Test secondary ranking\n",
        "print(\"Testing secondary ranking with query/title overlap boost...\")\n",
        "\n",
        "test_query = \"single serve K Cup coffee maker for K-Cup pods\"\n",
        "print(f\"\\nQuery: '{test_query}'\")\n",
        "\n",
        "# With secondary ranking - show query overlap boost effect (able to see it from un-boosted results)\n",
        "print(\"\\n-------  With Secondary Ranking (query overlap)   ------\")\n",
        "ranked_results = hybrid_search_with_ranking(test_query, df_baseline_v1, table_v1, model_v1, light_keywords,\n",
        "                    alpha=0.7, beta=0.3, keyword_threshold=0.1, top_k=5)\n",
        "for idx, row in ranked_results.iterrows():\n",
        "    print(f\"Score: {row['score']:.3f} | {row['product_title'][:70]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Secondary ranking with brand boost or query overlap is powerful to coerce search results to user preference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. HYBRID SEARCH PERFORMANCE:\n",
            "   Pure Semantic:     HITS@1=0.962, MRR=0.972\n",
            "   Hybrid (Light):    HITS@1=0.981, MRR=0.981\n",
            "   Hybrid (Moderate): HITS@1=0.981, MRR=0.981\n",
            "\n",
            "2. TEXT COMBINATION STRATEGY:\n",
            "   Strategy V1 (Original Order - title + description + bullets + brand + color)\n",
            "   Winner with HITS@1=0.962 vs V2=0.943, V3=0.943\n",
            "\n",
            "3. BASELINE vs IMPUTED DATA:\n",
            "   Semantic improvement: +0.000 HITS@1\n",
            "   Hybrid improvement:   +0.000 HITS@1\n",
            "\n",
            "4. WEIGHT OPTIMIZATION RESULTS:\n",
            "   All hybrid combinations (α=0.5-0.9): HITS@1=0.981\n",
            "   Pure semantic (α=1.0): HITS@1=0.962\n",
            "   Conclusion: Any hybrid ratio works equally well\n",
            "   Chosen: α=0.7, β=0.3 (semantic priority + keyword enhancement)\n",
            "\n",
            "5. SECONDARY RANKING:\n",
            "   - Successfully demonstrated brand boosting (Hamilton Beach, CHEFMAN)\n",
            "   - Shows business logic can override pure relevance scores\n",
            "   - Ready for production customization (margins, partnerships, etc.)\n",
            "\n",
            "6. PRODUCTION RECOMMENDATIONS:\n",
            "   - Use hybrid search: +0.019 HITS@1 improvement (96.2% → 98.1%)\n",
            "   - Text combination: V1 Original Order (winner vs V2/V3)\n",
            "   - Dataset: Baseline (imputation showed +0.000 improvement)\n",
            "   - Weights: α=0.7, β=0.3 (semantic priority, keyword enhancement)\n",
            "   - Keyword threshold: 0.1 (avoid weak matches)\n",
            "   - Secondary ranking: Available for business logic customization\n"
          ]
        }
      ],
      "source": [
        "# Finalize notebook findings and save results\n",
        "print(\"\\n1. HYBRID SEARCH PERFORMANCE:\")\n",
        "print(f\"   Pure Semantic:     HITS@1={semantic_metrics['hits_at_1']:.3f}, MRR={semantic_metrics['mrr']:.3f}\")\n",
        "print(f\"   Hybrid (Light):    HITS@1={hybrid_light_metrics['hits_at_1']:.3f}, MRR={hybrid_light_metrics['mrr']:.3f}\")\n",
        "print(f\"   Hybrid (Moderate): HITS@1={hybrid_moderate_metrics['hits_at_1']:.3f}, MRR={hybrid_moderate_metrics['mrr']:.3f}\")\n",
        "\n",
        "print(\"\\n2. TEXT COMBINATION STRATEGY:\")\n",
        "print(\"   Strategy V1 (Original Order - title + description + bullets + brand + color)\")\n",
        "print(\"   Winner with HITS@1=0.962 vs V2=0.943, V3=0.943\")\n",
        "\n",
        "print(\"\\n3. BASELINE vs IMPUTED DATA:\")\n",
        "print(f\"   Semantic improvement: {semantic_improvement:+.3f} HITS@1\")\n",
        "print(f\"   Hybrid improvement:   {hybrid_improvement:+.3f} HITS@1\")\n",
        "\n",
        "print(\"\\n4. WEIGHT OPTIMIZATION RESULTS:\")\n",
        "print(f\"   All hybrid combinations (α=0.5-0.9): HITS@1=0.981\")\n",
        "print(f\"   Pure semantic (α=1.0): HITS@1=0.962\") \n",
        "print(f\"   Conclusion: Any hybrid ratio works equally well\")\n",
        "print(f\"   Chosen: α=0.7, β=0.3 (semantic priority + keyword enhancement)\")\n",
        "\n",
        "print(\"\\n5. SECONDARY RANKING:\")\n",
        "print(\"   - Successfully demonstrated brand boosting (Hamilton Beach, CHEFMAN)\")\n",
        "print(\"   - Shows business logic can override pure relevance scores\")\n",
        "print(\"   - Ready for production customization (margins, partnerships, etc.)\")\n",
        "\n",
        "print(\"\\n6. PRODUCTION RECOMMENDATIONS:\")\n",
        "print(\"   - Use hybrid search: +0.019 HITS@1 improvement (96.2% → 98.1%)\")\n",
        "print(\"   - Text combination: V1 Original Order (winner vs V2/V3)\")\n",
        "print(\"   - Dataset: Baseline (imputation showed +0.000 improvement)\")\n",
        "print(\"   - Weights: α=0.7, β=0.3 (semantic priority, keyword enhancement)\")\n",
        "print(\"   - Keyword threshold: 0.1 (avoid weak matches)\")\n",
        "print(\"   - Secondary ranking: Available for business logic customization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to results directory\n",
        "final_results = {\n",
        "    'hybrid_search_results': {\n",
        "        'pure_semantic': semantic_metrics,\n",
        "        'hybrid_light': hybrid_light_metrics,\n",
        "        'hybrid_moderate': hybrid_moderate_metrics\n",
        "    },\n",
        "    'imputation_impact': {\n",
        "        'semantic_improvement': semantic_improvement,\n",
        "        'hybrid_improvement': hybrid_improvement\n",
        "    },\n",
        "    'weight_optimization': {\n",
        "        'all_hybrid_performance': 0.981,\n",
        "        'pure_semantic_performance': 0.962,\n",
        "        'chosen_alpha': 0.7,\n",
        "        'chosen_beta': 0.3,\n",
        "        'reasoning': 'semantic_priority_with_keyword_enhancement'\n",
        "    },\n",
        "    'production_config': {\n",
        "        'text_combination': 'strategy_v1_original_order',\n",
        "        'embedding_model': 'all-MiniLM-L6-v2',\n",
        "        'keywords_type': 'light',\n",
        "        'keyword_threshold': 0.1,\n",
        "        'use_imputed_data': semantic_improvement > 0,\n",
        "        'weights': {'alpha': 0.7, 'beta': 0.3},\n",
        "        'secondary_ranking_available': True\n",
        "    }\n",
        "}\n",
        "\n",
        "os.makedirs(\"../src/results/hybrid_search\", exist_ok=True)\n",
        "with open(\"../src/results/hybrid_search/hybrid_search_evaluation.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
